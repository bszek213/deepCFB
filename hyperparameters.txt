num_layers: 2
kernel_initializer: random_normal
l2_reg: 2.489654034499668e-06
use_batch_norm: True
units_layer_0: 416
activation_layer_0: relu
dropout_layer_0: 0.30000000000000004
optimizer: rmsprop
learning_rate: 0.000471321317072639
batch_size: 32
decay_steps: 5000
decay_rate: 0.8
clipnorm: 0.9
units_layer_1: 200
activation_layer_1: tanh
dropout_layer_1: 0.30000000000000004
units_layer_2: 464
activation_layer_2: relu
dropout_layer_2: 0.4
units_layer_3: 176
activation_layer_3: tanh
dropout_layer_3: 0.4
units_layer_4: 320
activation_layer_4: tanh
dropout_layer_4: 0.2
units_layer_5: 392
activation_layer_5: tanh
dropout_layer_5: 0.2
units_layer_6: 128
activation_layer_6: sigmoid
dropout_layer_6: 0.1
units_layer_7: 224
activation_layer_7: relu
dropout_layer_7: 0.0
units_layer_8: 104
activation_layer_8: relu
dropout_layer_8: 0.30000000000000004
momentum: 0.1
units_layer_9: 224
activation_layer_9: relu
dropout_layer_9: 0.2
best_batch_size: 32
